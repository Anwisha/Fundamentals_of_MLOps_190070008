Part 1:
1] AIOps, (for artificial intelligence for IT operations) is the application of artificial intelligence (AI) to enhance IT operations. Specifically, AIOps uses big data, analytics, and machine learning capabilities to do the following:
Collect and aggregate the huge and ever-increasing volumes of operations data generated by multiple IT infrastructure components, applications, and performance-monitoring tools
Intelligently sift ‘signals’ out of the ‘noise’ to identify significant events and patterns related to system performance and availability issues.
Diagnose root causes and report them to IT for rapid response and remediation—or, in some cases, automatically resolve these issues without human intervention.
By replacing multiple separate, manual IT operations tools with a single, intelligent, and automated IT operations platform, AIOps enables IT operations teams to respond more quickly—even proactively—to slowdowns and outages, with a lot less effort.

AIOps is all about supporting and reacting to its issues in real-time and providing analytics to your operations teams. These functions include performance monitoring, event analysis, correlation, and IT automation.
MLOps, on the other hand, focuses on managing training and testing data that is needed to create machine learning models effectively. It is all about monitoring and management of ML models. It focuses on the Machine Learning operationalization pipeline.
ML -
- Multi-source data consumption
– Source Code Control
– Deployment and Test Services-Tracking ML model using metadata
– Automate ML experiments
– Mitigate risks and bias in model validation
AI - 
– Application Monitoring-Automating manual or repetitive processes
– Anomaly Detection
– Predictive maintenance
– Incident management


References:
https://betterprogramming.pub/mlops-vs-aiops-6e5354704dab
https://neptune.ai/blog/mlops-vs-aiops-differences

2] A linear regression model predicts the target as a weighted sum of the feature inputs.
The interpretation of a weight in the linear regression model depends on the type of the corresponding feature.
  Numerical feature: Increasing the numerical feature by one unit changes the estimated outcome by its weight. An example of a numerical feature is the size of a house.
  Binary feature: A feature that takes one of two possible values for each instance. An example is the feature "House comes with a garden". One of the values counts as the reference category (in some programming languages encoded with 0), such as "No garden". Changing the feature from the reference category to the other category changes the estimated outcome by the feature's weight.
  Categorical feature with multiple categories: A feature with a fixed number of possible values. An example is the feature "floor type", with possible categories "carpet", "laminate" and "parquet". A solution to deal with many categories is the one-hot-encoding, meaning that each category has its own binary column. For a categorical feature with L categories, you only need L-1 columns, because the L-th column would have redundant information (e.g. when columns 1 to L-1 all have value 0 for one instance, we know that the categorical feature of this instance takes on category L). The interpretation for each category is then the same as the interpretation for binary features. 
  Intercept b0: The intercept is the feature weight for the "constant feature", which is always 1 for all instances. Most software packages automatically add this "1"-feature to estimate the intercept. The interpretation is: For an instance with all numerical feature values at zero and the categorical feature values at the reference categories, the model prediction is the intercept weight. The interpretation of the intercept is usually not relevant because instances with all features values at zero often make no sense. The interpretation is only meaningful when the features have been standardised (mean of zero, standard deviation of one). Then the intercept reflects the predicted outcome of an instance where all features are at their mean value.
  
  
  
  Part 2]
  
  
  
  
  

